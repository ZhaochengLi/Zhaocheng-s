In the development of the theory of computing, we have presented several models of coputing devices:

1. Finite automata are good models for devices that have a small amount of memory;

2. Pushdown automata are models for devices that have an unlimited memory that is useable ONLY in the last in, first out manner of a stack.

However, we have shown that some very simple tasks are beyond the capabilities of these models. Hence they are too restricted to serve as models of general purpose computers.



CHAPTER THREE: TURING MACHINES

$3.1 Turing Machines$

A much more powerful model, firstly propised by Alan Turing in 1936, called the 'Turing Machine'.

It is similar to a finite automata but with an unlimited and unrestricted memory.

A Turing Machine is a much more accurate model of a general purpose computer. A TM can do everything that a real computer can do.(Nonetheless, even a TM cannot solve certain problems. In a very real sense, these problems are beyond the theoretical limits of computations).

--CHARACTERS:
	1. It uses an infinite tape as its unlimited memory.	
	2. It has a tape head that can READ AND WRITE symbols and move around on the tape.
	3. Initially the tape contains only the input string and is blank everywhere else; if the machine needs to store information, it may write this information on the tape.
	4. To read the information that it has written, the machine can move its head back over it.
	5. The machine continues computing until it decides to produce an output. The OUTPUT (ACCEPT and REJECT) are obtained by entering designated accepting and rejecting states
	6. If it does not enter an accepting or a rejecting state, it will go on forever, NEVER HALTING.

--DIFFERENCES BETWEEN FAs AND TMs:
	1. the TM can both write on the tape and read from it.
	2. the read-write head can move both to the left and to the right.
	3. the tape is infinite.
	4. the special states for rejecting and accepting take effect immediately.

--FORMAL DEFINITION OF A TURING MACHINE:
	- the heart of the definition is the transition function beacause it tells us how the machine gets from one step to the next.
	- δ takes the form: Q × Γ −→ Q × Γ × {L,R}.
	- For example, δ(q, a) = (r, b, L), meaning the machine is in a certain q and the head is over a tape square containing a symbol a, and the machine is about to write the symbol b replacing the a, and goes to state r. The 3rd component is either L or R indicating whether the head moves to the left or right after writing.

	- A Turing Machine is a 7-tuple, (Q, Σ, Γ, δ, q0, q-accept, q-reject), where Q, Σ, Γ are finite sets and:
		1. Q is the set of states;
		2. Σ is the input alphabet not containing the blank symbol u;
		3. Γ is the tape alphabet, where u ∈ Γ and Σ ⊆ Γ;
		4. δ : Q × Γ−→ Q × Γ × {L,R} is the transition function,
		5. q0 ∈ Q is the start state,
		6. q-accept ∈ Q is the accept state, and
		7. q-reject ∈ Q is the reject state, where qreject != qaccept.
		
	- A Turing machine M = (Q, Σ, Γ, δ, q0, q-accept, q-reject) computes as follows.
		Initially,
			1. M receives its input w = w1w2w3...wn ∈ Σ∗ on the leftmost n squares of the tape, and the rest of the tape is blank(i.e. filled with blank symbols)
			
			2. the head starts on the leftmost square of the tape. Note that Σ does not contain the blank symbol, SO THE FIRST BLANK APPEARING ON THE TAPE MARKS THE END OF THE INPUT.
			
			3. Once M has started, the computation proceeds according to the rules described by the transition function. If M ever tries to move its head to the left off the left-hand end of the tape, the head stays in the same place for that move, even though the transition function indicates L.
			
			4. The computation continues until it enters either the accept or reject states, at which point it halts. If neither occurs, M goes on forever.
			
			5. as a Turing machine computes, changes occur in the current state, the current tape contents, and the current head location. A setting of these three items is called a CONFIGURATION of the Turing machine. Configurations often are represented in a special way. For a state q and two strings u and v over the tape alphabet Γ, we write uqv for the configuration where the current state is q, the current tape contents is uv, and the current head location is the first symbol of v. The tape contains only blanks following the last symbol of v. For example, 1011(q7)01111 represents the configuration when the tape is 101101111. the current state is q7, and the head is currently on the second 0.
			
			6. Saying that the configuration C1 YIELDS configuration C2 of the Turing machine can legally go from C1 to C2 in a single step.
				- For example, we have a, b, and c in Γ, as well as u and v in Γ∗ and states qi and qj. In that case, ua(qi)bv and u(qj)acv are two configurations. Say that: ua(qi)bv yields u(qj)acv if δ(qi, b) = (qj, c, L).
				- Similarly, ua(qi)bv yields uac(qj)v if δ(qi, b) = (qj, c, R).	
				
				- Special cases occur when the head is at one of the ends of the configuration. For the left-hand end, the configuration (qi)bv yields (qj)cv if the transition function is left-moving (because we prevent the machine from going off the left-hand end of the tape), and it yields c(qj)v for the right-moving transition. For the right-moving end, the configuration ua(qi) is equivalent to ua(qi)u because we assume that blanks follow the part of the tape represented in the configuration. Thus we can handle this case as before, with the head no longer at the right-hand end.
				
				- The start configuration of M on input w is the configuration q0w, which indicates that the machine is in the start state q0 with its head at the leftmost position on the tape.
				
				- In an accepting configuration, the state of the configuration is q-accept; In a rejecting configuration, the state of the configuration is q-reject.
				
				- Accepting and rejecting configuration are HALTING CONFIGURATIONS and do not yield further configuration. Because the machine is defined to halt when in the states q-accept and q-reject, we equivalently could have defined the transition function to have the more complicated form δ : Q′ × Γ −→ Q × Γ × {L,R}, where Q′ is Q without q-accept and q-reject.	
			
			7. A Turing machine M accepts input w if a sequence of configurations C1, C2, ..., CK exists, where
				- C1 is the start configuration of M on input w,
				- each Ci yields Ci+1, and
				- CK is an accepting configuration.
			   the collection of strings that M accepts is the language of M, or the language recognized by M, denoted L(M).
			
	- DEFINITION 3.5: Call a language Turing-recognizable if some Turing machine recognizes it. (or recursively enumerable language)
			
	- When we start a Turing machine on an input, three outcomes are possible. The machine may accept, reject, or loop. By loop we wean that the machine simply does not halt. Looping may entail any simple or complex behavior that never leads to a halting state.
			
	- A Turing machine M can fail to accept an input by entering the q-reject state and rejecting, or by looping. Sometimes distinguishing a machine that is looping from one that's merely taking a long time is difficult. For this reason, we prefer Turing machines that halt on all inputs; such machine never loop. These machines are called DECIDERS because they always make a decision to accept or reject. A decider that recognizs some language also is said to decide that language.
			
	- DEFINITION 3.6: Call a language Turing-decidable or simply decidable if some Turing machine decides it. (or recursive language)
			
	- EVERY DICIDABLE LANGUAGE IS TURING-RECOGNIZABLE.
	- NOT ALL TURING-RECOGNIZABLE LANGUAGES ARE DECIDABLE.
			
$3.2 Variants of Turing Machines$
	
-- Alternative definitiond of Turing machines includes versions with multiple tapes or with nondeterminism. They are called variants of the Turing machine model.

	- The original model and its reasonable variants all have the same power -- they reognize the same class of languages. In this section, we describe some of these variants and the proofs of equivalence in power. We call this invariance to certain changes in the definition ROBUSTNESS.
	- Both finite automata and pushdown automata are somehow robust models, but Turing machines have an astonishing degree of robustness.
	
	- To illustrate the robustness of the Turing machine model, let's vary the type of transition function permitted. Suppose we allow the Turing machine the ability to stay put. The transiton function would then have the form δ : Q × Γ −→ Q × Γ×{L,R, S}. Might this featre allow Turing machine to recognize additonal languages, thus adding to the power of the model?
	
	- It is NOT. Because we can convert any TM with the "stay put" feature to one that does not have it. We do so by replacing each stay put transition with two transitions: one that moves to the right and the second back to the left.
	
	- The key of showing the equivalence of TM variants is that to show one can simulate the other.

-- Multiple Turing Machines
	- A multiple Turing machine is like an ordinary Turing machine with several tapes. Each tape has its own head for reading and writing. 
	- Initially the input appears on tape 1, and the others start out blank. The transition function is changed to allow for reading, and moving the heads on some or all of the tapes simultaneously. Formally, it is δ : Q × Γk −→ Q × Γk × {L,R, S}k, where k is the number of tapes. The expression δ(qi, a1, ..., ak) = (qj, b1,..., bk, L, R,...,L) means that if the machine is in state qi and heads 1 through k are reading symbols a1 through ak, the machine goes to state qj, writes symbols b1 through bk, and directs each head to move left or right, or to stay put, as specified.
	
	- Multiple Turing machines appear to be more powerful than ordinary Turing machines, but we can show that they are equivalent in power. Recall that two machines are equivalent if they recognize the same language.
	
	- Theorem 3.13: Every multiple Turing machine has an equivalent single-tape Turing machine.
	
	- Prove by construction: we show to convert a multiple TM M to an equivalent single-tape TM S. The key idea is to show how to simulate M with S.
	
		- say that M has k tapes. Then S simulates the effect of k tapes bt storing ther information on its singlr tape. It uses the new symbol # as a delimiter to separate the contents of the difference tapes. In addition to the contents of these tapes, S must keep track of the locations of the heads. It does so by writing a tape symbol with a dot above it to mark the place where the head on that tape would be. Think of these as "virtual" tapes and heads. As before, the "dotted" tape symbols are simply new symbols that have been added to the tape alphabet.
	- Representing three tapes with one
		- S = "On input w=w1...wn"
			1. First S inputs its tape into the format that represents all k tapes of M. The formatted tape contains
				
					"#w1(dot)w2 ... wn #u(dot)#u(dot)# ... #".
					
			2. To simulate a single move, S scans its tape from the first #, which marks the left-hand end, to the (k+1)st #, which marks the right-hand end, in order to determine  the symbols under the virtual heads. 
				Then S makes a second pass to update the tapes according
				to the way that M's transition function dictates.
			
			3. If at any point S moves one of the virtual heads to the right onto a #, this action signifies that M has moved the corresponding head onto the previously unread blank portion of the tape. So S writes a blank symbol on this tape cell and shifts the tape contents, from this cell until the rightmost #, one unit to the right. Then it continues the simulation as before.
	
	- Corollary 3.15: A language is Turing-recognizable if and only if some multiple Turing machine recognizes it.
		- PROOF: A Turing-recognizable language is recognized by an ordinary(single-tape) Turing machine, which is a special case of a multiple Turing machine. That proves one direction of this corollary. The pther direction follows from Theorem above.
	
-- NONDETERMINISM TURING MACHINES
	- A NTM is defined in the expected way. At any point in a computation, the machine may proceed according to several possibilities. The transition function for a NTM has the form δ : Q × Γ −→ Power_set(Q × Γ × {L,R}).
	
	- The computation of a NTM is a tree whose branches correspond to different possibilities for the machine. If some branch of the computation leads to the accept state, the machine accepts its input. Feel free to review NONDETERMINISM in Secntion 1.2.
	
	- Similarly, the nondeterminism doee not effect the power of the Turing machine model.
	
	- Theorem 3.16: Every nondeterministic Turing machine has an equivalent deterministic Turing machine.
	
	- Prove by construction as well. We can simulate any nondeterministic TM N with a deterministic TM D. The idea behind the simulation is to have D try ALL possible branches of N's nondeterministic computation. If D ever finds the accept state on one of these branches, D accepts. Otherwise, D's simulation will not terminate.
	
	- We view N's computation on an input w as a tree. Each branch of the tree represents one of the branches of the nondeterminism. Each node of the tree is a configuration of N. The root of the tree is the start configuration. The TM D searches this tree for an accepting configuration. Conducting this search carefully is crucial lest(in case) D fail to visit the entire tree. A tempting, though bad, idea is to have D explore the tree by using depth-first search. The depth-first search strategy goes all the way down one branch before backing up to explore other branches. If D were to explore the tree in this manner, D could go forever down one infinite branch and miss an accepting configuration on some other branch. Hence we design D to explore the tree by using breadth-first search instead. This strategy explores all branches to the same depth before going on to explore any branch to the next depth. This method guarantees that D will visit every node in the tree until it encounters an accepting configuration.
	
	- Example: the simulating deterministic TM D has three tapes. By Theorem above, this arrangement is equivalent to having a single tape. The machine uses its three tapes in a partucular way. Tape 1 always contains the input string and is never altered. Tape 2 maintains a copy of N's tape on some branch of its nodeterminism computation. Tape 3 keeps track of D's location in N's nondeterministic computation tree.	
	
		- Deterministic TM D simulating nondeterministic TM N.
		- Consider the data representation on tape 3. Every node in the tree can have at most b children, where b is the size of the largest set of possible choices given by N's transition function.
		- To every node in the tree we assign an address that is a string over the alphabet Γb = {1, 2, . . . , b}. We assign the address 231 to the node we arrive at by starting at the root, going to its 2nd child, goingto that node's 3rd child, and finally going to that node's 1st child. Each symbol in the string tells us which choice to make next when simulating a step in one branch in N's nondeterministic computation. Sometimes a symbol may not correspond to any choice if too few choices are available for a configuration. In that case, the address is invalid and does not correspond to any node. Tape 3 contains string over Γb. It represents the branch of N's computation from the root to the node addressed by that string unless the address is invalid. The empty string is the address of the root of the tree.
		
		- Now we describe D. 
			1. Initially, tape 1 contains the input w, and tapes 2 and 3 are empty.
			
			2. Copy tape 1 to tape 2 and initialize the string on tape 3 to be ε.
			
			3. Use tape 2 to simulate N with input w on one branch of its nondeterministic computation. Before each step of N, consult the next symbol on tape 3 to determine which choice to make among those allowed by N's transition function. If not more symbols remain on tape 3 or if this nondeterministic choice is invalid, abort this branch by going to stage 4. Also go to stage 4 if a rejecting configuration is encountered. If an accepting configuration is encountered, accept the input.
			
			4. Replace the string on tape 3 with the (lexicographically) next string in the string ordering. Simulate the next branch of N's computation by going to stage 2.
		
	- Corollary 3.18: A langugage is Turing-recognizable if and only if some nondeterministic Turing machine recognizes it.
		- PROOF: Any deterministic TM is automatically a nondeterministic TM, and so one direction of this corollary follows immediately. The other direction follows from Theorem 3.16
	- We can modify the proof of Theorem 3.16 so that if N always halts on all branches of its computation, D will always halt. We call nondeterministic Turing machine a decider if all branches halt on all inputs.
	
	- Corollary 3.19: A language is decidable if and only if some nondeterministic Turing machine decides it.
	
-- ENUMERATORS:
	- Some prople use the term 'recursively enumerable language' for Turing-recognizable language. That term originates from a type of Turing machine variant called an enumerator. Loosely defined, an enumerator is a Turing machine with an attached printer. The Turing machine can use that printer as an output device to print strings. Every time the Turing machine wants to add a string to the list, it sends the string to the printer.
	
	- An enumerator E starts with a blank input on its work tape. If the enumerator does not halt, it may print an infinite list of strings. The language enumerated by E is the collection of all the strings that is eventually prints out. Moreover, E may generate the strings of the language in any order, possibly with repititions. 
	
	- Theorem 3.21: A language is Turing-recognizable if and only if some enumerator enumerates it.
	
	- Prove by construction. 
		- First we show that if we have an enumerator E that enumerates a language A, a TM M recognizes A. The TM M works in the following way.
		- M = "On input w:
			1. Run E. Every time that E outputs a string, compare it with w.
			2. If w ever appears in the output of E, accept."
		- Clearly, M accepts those strings that appear on E's list.
		- Now we do the other direction. If M recognizes a language A, we can construct the following enumerator E for A. Say that s1, s2, s3, ... is a list of all possible strings in Σ∗
		- E = "Ignore the input.
			1. Repeat the following for i = 1,2,3,...
			2. Run M for i steps on each input, s1,s1,...,si
			3. If any computation accept, print out the corresponding sj."
		- If M accepts a particular string s, eventually it will appear on the list generated by E. In fact, it will appear on the list infinitely many times because M runs from the begining on each string for each repitition of step 1. This procedure gives the effect of running M in parallel on all possible input strings.
		
-- Simulate a TM by a computer
	- Use a table of transitions to look up changes in state
	- How to simulate infinite tape?
		- Keep disks in two stacks. The further down the stack, the further away from the location of the TM head.

-- Simulate a Computer by a TM
	- Assumptions about a computer:
		- Storage: unlimited length words, and address 0, 1, 2, ...
		- Computer program is stored in words in memory. indirect addressing is permitted.
		- Each instruction contains a finite number of words and changes the value of at most one word.
		- Any operation can be performed on any word. (No need for registers.)
	- About the TM
		- Use multitape TM, with following tapes:
		
		- Tape 1, start of tape symbol followed by the address, followed by the word at that address. The addresses are ordered 0, 1, 2, etc.
		- T2, instruction counter holds address.
		- T3, memory address or contents of memory address.
		- T4, computer's input file
		- T5, scratch
		
-- Simulating an Instruction Cycle
	- Search the first tape for the address matching the instruction counter
	
	- When the instruction address is found, its value determines an operation and possibly another address containing a value.
	
	- If the instruction requires a value of some address, then copy that address onto third tape and find its value and copy it onto the third tape.
	
	- Execute the instruction using that value.
		– If more space in a word is needed, the entire nonblank tape to the right must be shifted over.
		– Example: Jump.
		
	- After performing the instruction, if it’s not a jump, add 1 to the instruction counter and begin again.
	
-- Running Time
	- Can we do this efficiently?
	- We can’t allow multiplication of arbitrarily large numbers in one operation. Otherwise, the computer could square a number at every step, starting with 2, and after n steps it would have a number of size 2^2^n, which would have 2^n bits and require exponential time for the TM.
	
	- So either limit wordsize (e.g., to 64 bits,) or allow words to grow only by one bit with each operation.
	
	- In our simulation, after t(n) steps by the computer, the largest word is t(n) bits, so t(n) cells are needed to represent the computer word in the TM, and the nonblank tape is t(n)^2 long.
	
	- Then each lookup and each shift takes the TM only t(n)^2 steps, and in general, for usual instructions, there’s a cost of only t(n)^2 per step of the computer.
	
	- t(n) steps of the computer can be simulated with t(n)^3 steps on the TM
	
	- Then the single tape TM can simulate the computer with t(n)^6 steps.

-- Simulation Overhead
	- It is important to understand the cost of a simulation, in terms of time and space. E.g., when simulating a k-tape machine, each simulation step requires two whole scans of the tape – one to read the k tape-head positions, and one to update. The update could involve k shifts, which could require moving up to the whole tape.
	- We measure costs as a function of input size n.
	- If the original machine takes time O(t(n)) for some time function t, it could use up to O(t(n)) tape cells, so the simulation tape has O(t(n)) cells. So each simulated step takes O(t(n)) + O(t(n)) + k · O(t(n)) = O(t(n)) simulation steps. Since there are O(t(n)) steps to simulate, the entire simulation takes time O(t(n)^2).

-- Coding Structured Inputs to TMs
	- TM Input = string
	
	- We would like to consider TMs that take other objects, such as numbers, graphs, automata, TMs (and combinations thereof) as inputs
	- We can represent these objects as strings
	
	- For an object O we write ⟨O⟩ to denote its representation
	
	- For a sequnce O1,...,Ok we write ⟨O1,...,Ok⟩ to denote its representation
	
	- When a TM expecting a certain type of object gets a string w is input, it must first check whether the string has the right format (i.e. w = ⟨O⟩ for an object O) and then must decode it into some internal representation
	
- Example: TM for Graph Connectivity
	- A TM which decides the language A = {⟨G⟩ | G is a connected undirected graph}.
	
	- The encoding of a graph ⟨G⟩ is a list of nodes followed by a list of edges, e.g., ⟨G⟩ = (1, 2, 3, 4), ((1, 2), (2, 3), (1, 3), (1, 4))
	
	- Easiest might be to assume a three tape machine, with the input on the first tape.
	
	- M checks that the input has the correct format
	
	- IDEA: Tape 1 stores the graph. Tape 2 stores the queue of nodes to explore in a BFS. Tape 3 is a work tape.
	
	- M copies the first node’s name on Tape 2. While Tape 2 is not blank:
		1. M scans each unmarked edge on tape 1 and tries to match an endpoint of the edge to the front of the queue (leftmost node name on the second tape).
		2. If it matches, the edge is marked and M puts the other endpoint at the right end of Tape 2’s list.
		3. When M completes this scan, it removes the node at the start of the queue and marks it on Tape 1.
		
	- M scans Tape 1 to see if every node is marked. If it is M accepts; else it rejects.

	
	
	



